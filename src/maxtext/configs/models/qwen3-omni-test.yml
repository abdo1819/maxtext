# Tiny Qwen3-Omni model config for testing GRPO audio pipeline on v4-8.
# Same decoder_block and structure as qwen3-omni-30b-a3b but with drastically
# reduced dimensions so the model fits in memory on a single v4-8 slice.

decoder_block: "qwen3_moe"

# Drastically reduced text decoder
base_emb_dim: 256
base_mlp_dim: 512
base_num_query_heads: 4
base_num_kv_heads: 2
base_num_decoder_layers: 2
head_dim: 64
mlp_activations: ["silu", "linear"]
vocab_size: 152064            # keep original tokenizer vocab
normalization_layer_epsilon: 1.0e-6
use_qk_norm: True

# Minimal MoE — 2 experts so it's divisible by 2-device inference mesh
num_experts: 2
num_experts_per_tok: 1
norm_topk_prob: false

# RoPE
rope_max_timescale: 1_000_000
max_position_embeddings: 4096

enable_dropout: False

# Vision encoder — reduced
use_multimodal: true
image_size_for_vit: 224
hidden_size_for_vit: 256
intermediate_size_for_vit: 512
num_attention_heads_for_vit: 4
num_hidden_layers_for_vit: 2
num_channels_for_vit: 3
patch_size_for_vit: 16
temporal_patch_size_for_vit: 2
spatial_merge_size_for_vit: 2
out_hidden_size_for_vit: 256
num_position_embeddings_for_vit: 196
deepstack_visual_indexes_for_vit: []

# Audio encoder — reduced but structurally compatible
use_audio: true
d_model_for_audio: 128
encoder_layers_for_audio: 2
encoder_attention_heads_for_audio: 4
encoder_ffn_dim_for_audio: 256
max_source_positions_for_audio: 1500
num_mel_bins_for_audio: 128
downsample_hidden_size_for_audio: 128
output_dim_for_audio: 256
attention_dropout_for_audio: 0.0
n_window_for_audio: 50
n_window_infer_for_audio: 400
conv_chunksize_for_audio: 500
num_conv_layers_for_audio: 3
max_timescale_for_audio: 10000.0
max_sample_len_for_audio: 10000

# MRoPE
use_mrope: true
mrope_section: [24, 20, 20]
position_id_per_seconds: 25
