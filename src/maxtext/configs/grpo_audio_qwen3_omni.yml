# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Audio GRPO Configuration for Qwen3-Omni-30B-A3B
# Used with the standalone experimental GRPO trainer:
#   python3 -m MaxText.experimental.rl.grpo_trainer configs/grpo_audio_qwen3_omni.yml

base_config: "base.yml"
model_name: "qwen3-omni-30b-a3b"

# ====== Multimodal / Audio ======
use_multimodal: true
use_audio: true
freeze_audio_encoder_params: true
freeze_vision_encoder_params: true

# ====== GRPO Settings ======
use_grpo: true
num_generations: 4
grpo_beta: 0.04
grpo_epsilon: 0.2

# Chain-of-thought format tokens for ASR
reasoning_start_token: '<reasoning>'
reasoning_end_token: '</reasoning>'
solution_start_token: '<answer>'
solution_end_token: '</answer>'

# Reward parameters for ASR
reward_exact_format_match: 3.0
asr_reward_cap: 2.0
asr_exact_match_bonus: 3.0
asr_format_bonus: 1.0
asr_no_answer_penalty: -2.0

# ====== Sequence Lengths ======
max_prefill_predict_length: 256
max_target_length: 1024

# ====== Data ======
dataset_type: grain
grain_file_type: arrayrecord
train_data_columns: 'ar'
# Override these via CLI or env:
# grain_train_files: "/path/to/audio_arrayrecord/"

# ====== Tokenizer ======
tokenizer_type: "huggingface"
# Override via CLI:
# tokenizer_path: "Qwen/Qwen3-Omni-30B-A3B-Instruct"

# ====== Inference / Rollout ======
decode_sampling_strategy: "weighted"
decode_sampling_temperature: 0.9
return_log_prob: true
inference_rollouts: 5
inference_devices_per_replica: 8
inference_replicas: 1

# ====== Training ======
weight_dtype: 'bfloat16'
attention: 'dot_product'
per_device_batch_size: 1
learning_rate: 1.0e-6
enable_dropout: false
steps: 1000
scan_layers: true
optimizer_memory_host_offload: true

add_bos: false
add_eos: false

# ====== Checkpointing ======
enable_checkpointing: true
async_checkpointing: false
checkpoint_period: 50
base_output_directory: "gs://arabic-asr-dataset/grpo_training"
# Override via CLI:
# load_parameters_path: "gs://path/to/converted/checkpoint"
